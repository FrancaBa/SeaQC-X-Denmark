########################################################################################################
## Written by frb for GronSL project (2024-2025)                                                      ##
## This is the main non-ML script for the QC. From here the different classes and methods are called. ##
########################################################################################################

import os
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import random
import builtins
import math

import source.helper_methods as helper
import source.qc_spike as qc_spike
import source.qc_interpolation_detector as qc_interpolated
import source.qc_shifts as qc_shifts
import source.qc_filling_missing_data as qc_fill_data
import source.qc_marker_probably_good as qc_prob_good
import source.qc_implausible_changes as qc_implausible_change
import source.qc_global_outliers as qc_global_outliers
import source.qc_stuck_values as qc_stuck_values
import source.data_extractor_monthly as extractor
import source.qc_test_results_to_mask as qc_test_results_to_mask
import source.tidal_signal_generation as tidal_signal_generation
import source.helper_nice_graphs as helper_nice_graphs

from sklearn.ensemble import IsolationForest
from sklearn.metrics import precision_score
from functools import reduce

class QualityFlagger():
    
    def __init__(self):

        self.helper = helper.HelperMethods()
        self.min_timestep = 1

        #Empty element to store different texts about QC tests and save as a summary document
        self.information = []

    def set_station(self, station):
        self.station = station
        self.information.append(['The following text summarizes the QC performed on measurements from ',station,':'])
    
    #provids details on station like coordinates
    def set_gauge_details(self, gauge_details_path):
        self.gauge_details_path = gauge_details_path

    #load config.json file to generate bitmask and flags after QC tests and to load threshold/parameters used for the QC tests
    def load_config_json(self, json_path):

        # Open and load JSON file containing the quality flag classification
        with open(json_path, 'r') as file:
            config_data = json.load(file)

        #Defines which flags are generated by the script
        self.qc_classes = config_data['qc_flag_classification']
        #Defines which bits are generated by the script
        self.qc_bit_definition = config_data['qc_binary_classification']
        #Defines which parameter potentially are used by the script (later on the timestep decides on the chosen constants)
        self.all_params = config_data['parameter_definition']
        #Defines which tests are active in the script
        self.active_tests = config_data['qc_active_tests']
        #Defines if certain tests should be carried out on detided data as well
        self.detide_mode = config_data['detide_mode']

    #Create output folder to save results
    def set_output_folder(self, folder_path):

        self.folder_path = folder_path

        #generate output folder for graphs and other docs
        if not os.path.exists(self.folder_path):
            os.makedirs(self.folder_path)

        self.helper.set_output_folder(self.folder_path)

    #set input table to correct heading
    def set_column_names(self, time_column, measurement_column, qc_column):
        self.time_column = time_column
        self.measurement_column = measurement_column
        self.qc_column = qc_column

    def set_missing_value_filler(self, missing_meas_value):
        #Dummy value for NaN-values in measurement series
        self.missing_meas_value = missing_meas_value 

    def import_data(self, path, file):
        """
        Importing the data from csv to df. And some first pre-processing:
        -Extracting areas of interest to csv filea. 
        -Fix timestamp column and column names 
        -Get resolution between different measurements
        -Plots for basic understanding
        -Remove invalid characters in measurement period
        -Set missing values to NaN
        already contains the format check and setting invalid formats to NaN

        Input:
        - path to file [str]
        - filename including ending [str]
        """
        if file.endswith(".csv"):
            #Open .csv file and fix the column names
            self.df_meas = pd.read_csv(os.path.join(path,file), sep=",", header=0)
            self.df_meas = self.df_meas[[self.time_column, self.measurement_column]]
            self.df_meas[self.time_column] = pd.to_datetime(self.df_meas[self.time_column], format='%Y%m%d%H%M%S')
        elif file.endswith(".dat"):
            #Open .dat file and fix the column names
            self.df_meas = pd.read_csv(os.path.join(path,file), sep="\\s+", header=None, names=[self.time_column, self.measurement_column, self.qc_column])
            self.df_meas = self.df_meas[[self.time_column, self.measurement_column]]
            self.df_meas[self.time_column] = pd.to_datetime(self.df_meas[self.time_column], format='%Y%m%d%H%M%S')
        elif file.endswith(".txt"):
            #Open .txt file and fix the column names
            self.df_meas = pd.read_csv(os.path.join(path,file), sep="\\s+", header=None, names=[self.time_column, self.measurement_column])
            self.df_meas[self.measurement_column] = self.df_meas[self.measurement_column]/100 #conversion from cm to m
            self.df_meas[self.time_column] = pd.to_datetime(self.df_meas[self.time_column], format='%Y%m%d%H%M')
        else:
            raise Exception('Format of the input measurement series is unkown to this script.')
        
        #Raise an error for data without two decimats when in meters -> Thresholds are currently defined for two decimals. User can adapt it based on the need
        # Count decimals and find the most frequent one
        decimal_counts = self.df_meas[self.measurement_column].astype(str).str.split('.').str[1].str.len().fillna(0).astype(int)
        most_frequent_decimal_count = decimal_counts.mode()[0]
        if most_frequent_decimal_count < 2:
            raise Exception('The input data series does not have at least two decimals. The code and thresholds are currently tuned for timeseries with at least 2 decimals. Please, adapt the config.json to run this script with the current timeseries.')

        #Extract data for manual labelling
        data_extractor = extractor.DataExtractor()
        data_extractor.set_output_folder(self.folder_path, self.station)
        data_extractor.set_missing_value_filler(self.missing_meas_value)
        data_extractor.run(self.df_meas, self.time_column, self.measurement_column)

        #drop seconds
        self.df_meas[self.time_column] = self.df_meas[self.time_column].dt.round('min')
        self.information.append(['The analyzed series goes from ',self.df_meas[self.time_column].iloc[0].strftime('%Y-%m-%d %H:%M:%S'),' to ', self.df_meas[self.time_column].iloc[-1].strftime('%Y-%m-%d %H:%M:%S'), '.'])

        #Length original dataset
        self.original_length = len(self.df_meas)

        # Extract the resolution in seconds, minutes, etc.
        # Here, we extract the time difference in minutes
        self.df_meas['time_diff'] = self.df_meas[self.time_column].diff()
        self.df_meas['resolution'] = self.df_meas['time_diff'].dt.total_seconds()/60
        frequent_timestep = self.df_meas['resolution'].value_counts().head(4)
        frequent_timestep_int =[int(res) for res in frequent_timestep.index]
        self.information.append(['The three most frequent time steps in dataset are:', frequent_timestep])
        print('The three most frequent time steps in dataset are:', frequent_timestep)
        self.min_timestep_int = reduce(math.gcd, frequent_timestep_int)
        self.min_timestep = str(float(self.min_timestep_int)) + 'min'
        #Select correct parameters based on timestep
        self.params = self.all_params.get(self.min_timestep)
        self.information.append(['The series is converted to a ', self.min_timestep, ' time resolution.'])
        print('The series is converted to a ', self.min_timestep, ' time resolution.')

        self.df_meas.loc[self.df_meas['resolution'] > 3600, 'resolution'] = np.nan
        self.information.append(['The measurement series is ', len(self.df_meas),'data entries long.'])

        #Plot the original ts to get an understanding
        self.helper.plot_df(self.df_meas[self.time_column][-2000:-1000], self.df_meas['resolution'][-2000:-1000],'step size','Timestamp ','Time resolution (zoomed)')
        self.helper.plot_df(self.df_meas[self.time_column][33300:33320], self.df_meas['resolution'][33300:33320],'step size','Timestamp ','Time resolution (zoomed 2)')
        self.helper.plot_df(self.df_meas[self.time_column], self.df_meas['resolution'],'step size','Timestamp ','Time resolution')

        #Initial screening of measurements:
        #1. Invalid characters: Set non-float elements to NaN
        # Count the original NaN values
        original_nan_count = self.df_meas[self.measurement_column].isna().sum()
        altered_ts = self.df_meas[self.measurement_column].apply(lambda x: x if isinstance(x, float) else np.nan)
        self.df_meas['incorrect_format'] = np.where(altered_ts, False, True)
        self.df_meas[self.measurement_column] = altered_ts
        # Count the new NaN values after the transformation -> Where there any invalid characters?
        new_nan_count = self.df_meas[self.measurement_column].isna().sum() - original_nan_count
        print('The measurement series contained',new_nan_count,'invalid data entires.')
        self.information.append(['The measurement series contains',new_nan_count,'invalid data entries.'])
        
        #2. Replace the missing values with nan
        self.df_meas['missing_values'] = self.df_meas[self.measurement_column] == self.missing_meas_value
        self.df_meas.loc[self.df_meas[self.measurement_column] == self.missing_meas_value, self.measurement_column] = np.nan
        ratio = (self.df_meas['missing_values'].sum()/len(self.df_meas))*100
        print(f'The measurement series contains {self.df_meas['missing_values'].sum()} missing data entires. This is {ratio}% of the overall dataset.')
        self.information.append([f'The measurement series contains {self.df_meas['missing_values'].sum()} missing data entries. This is {ratio}% of the overall dataset.'])
                                
        #Plot for visualisation
        self.helper.plot_df(self.df_meas[self.time_column], self.df_meas[self.measurement_column],'Water Level','Timestamp ','Measured water level')
        
        #Plot distribution for analysis
        plt.hist(self.df_meas[self.measurement_column] , bins=3000, edgecolor='black', alpha=0.7)
        plt.title('Distribution of Values')
        plt.xlabel('Water Level')
        plt.ylabel('Frequency')
        plt.savefig(os.path.join(self.folder_path,"Distribuiton - WL measurements.png"),  bbox_inches="tight")
        plt.close() 

        #Subsequent line makes the code slow, only enable when needed
        #print('length of ts:', len(self.df_meas))
        #self.helper.zoomable_plot_df(self.df_meas[self.time_column], self.df_meas[self.measurement_column],'Water Level','Timestamp ', 'Measured water level','measured water level')

    """
    The methods above are to open and preprocess the need information and data for the quality check.
    The run-method below is the core of the QC work. It calls the different QC steps, called detided_mode, converts the masks to a large bitmasks and assigns quality flags.
    """
    def run(self):
        #Set relevant column names
        self.adapted_meas_col_name = 'altered'
        suffix = '_detided'

        #Padding of ts to a homogenous timestep
        df = self.set_global_timestamp(self.df_meas)
        df[self.adapted_meas_col_name] = df[self.measurement_column]
        
        #Runs the different steps in the QC algorithm
        df = self.run_qc(df, self.adapted_meas_col_name)

        #Run the different steps in the QC algorithm for detided data
        if self.detide_mode == True:
            self.information.append(['The QC steps are now performed again for detided data.'])
            #1.Generate tidal signal and detided time series
            tidal_signal_construction = tidal_signal_generation.TidalSignalGenerator()
            tidal_signal_construction.set_station(self.station)
            tidal_signal_construction.set_gauge_details_path(self.gauge_details_path)
            tidal_signal_construction.set_output_folder(self.folder_path)
            df = tidal_signal_construction.run(df, self.adapted_meas_col_name, self.time_column, self.information)
            #2.Export the new series to csv files for manual labelling
            data_extractor = extractor.DataExtractor()
            data_extractor.set_output_folder(self.folder_path, self.station)
            data_extractor.set_missing_value_filler(self.missing_meas_value)
            data_extractor.run(df, self.time_column, 'detided_series', suffix)
            #3. Carry out QC steps on detided series as well
            df = self.run_qc(df, 'detided_series',suffix)

        #Probably good data
        #Mark all data as probably good data if it is only a short measurement period between bad data
        if self.active_tests['probably_good_data']:
            prob_good = qc_prob_good.ProbablyGoodDataFlagger()
            prob_good.set_output_folder(self.folder_path)
            prob_good.set_parameters(self.params)
            df = prob_good.run(df, self.adapted_meas_col_name, self.time_column, self.information, self.original_length)

        #Convert information of passed and failed tests to flags
        conversion_bitmask = qc_test_results_to_mask.QualityMasking()
        conversion_bitmask.set_output_folder(self.folder_path)
        conversion_bitmask.set_station(self.station)
        conversion_bitmask.set_flags(self.qc_classes)
        conversion_bitmask.set_bitmask(self.qc_bit_definition)
        conversion_bitmask.set_active_tests(self.active_tests)
        df = conversion_bitmask.convert_boolean_to_bitmasks(df)
        df = conversion_bitmask.convert_boolean_to_bitmasks(df, '_detided')
        df = conversion_bitmask.merge_bitmasks(df, self.detide_mode, self.information, suffix)
        df = conversion_bitmask.convert_bitmask_to_flags(df)
        conversion_bitmask.save_flagged_series(df, self.measurement_column, self.time_column, self.df_meas)

        #Save information from QC tests to txt file
        self.save_to_txt()

        #Make beautiful graphs for papers/presentations (not generalized)
        plotter = helper_nice_graphs.GraphMaker()
        plotter.set_output_folder(self.folder_path)
        plotter.set_station(self.station)
        plotter.run(df, self.time_column, self.measurement_column)

    def run_qc(self, df, relevant_measurements, suffix=''):
        """
        As main QC method, it calls the different steps taken in the QC approach. See commented text.

        Input:
        -Main dataframe [pandas df]
        -Column name of measurement series of interest [str]
        -suffix: ending for columns and graphs in order to run in different modes [str]
        """
        #Detect stuck values in ts
        stuck_values = qc_stuck_values.StuckValuesDetector()
        stuck_values.set_output_folder(self.folder_path)
        stuck_values.set_parameters(self.params)
        df = stuck_values.run(df, self.time_column, relevant_measurements, self.information, self.original_length, suffix)

        #Detect global outliers in ts
        global_outliers = qc_global_outliers.OutlierRemover()
        global_outliers.set_output_folder(self.folder_path)
        global_outliers.set_parameters(self.params, suffix)
        df = global_outliers.run(df, relevant_measurements, self.time_column, self.measurement_column, self.information, self.original_length, suffix)
        #df = global_outliers.run_zscore(df, relevant_measurements, self.time_column, self.measurement_column, self.information, self.original_length, suffix)

        #Detect interpolated values
        interpolated_qc = qc_interpolated.Interpolation_Detector()
        interpolated_qc.set_output_folder(self.folder_path)
        interpolated_qc.set_parameters(self.params)
        df = interpolated_qc.run_interpolation_detection(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)
       
        #Segmentation of ts in empty and measurement segments
        #Extract measurement segments and fill them accordingly
        self.segment_column = f'segments{suffix}'
        fill_data_qc = qc_fill_data.MissingDataFiller()
        fill_data_qc.set_output_folder(self.folder_path)
        fill_data_qc.set_parameters(self.params)
        df = fill_data_qc.segmentation_ts(df, relevant_measurements, self.segment_column, self.information, suffix)

        #Set short measurement periods between missing data periods as not trustworthy periods
        df = self.short_bad_measurement_periods(df, relevant_measurements, self.segment_column, suffix)

        #Add continuous helper columns
        df = fill_data_qc.polynomial_fill_data_column(df, relevant_measurements, self.time_column, self.segment_column, suffix)
        df = fill_data_qc.polynomial_fitted_data_column(df, relevant_measurements, self.time_column, self.segment_column, f'poly_interpolated_data{suffix}', suffix)
        df = fill_data_qc.spline_fitted_measurement_column(df, relevant_measurements, self.time_column, self.segment_column, suffix)
        #fill_data_qc.compare_filled_measurements(df, self.time_column, self.segment_column, suffix)

        #Detect implausible change rate over period
        implausible_change = qc_implausible_change.ImplausibleChangeDetector()
        implausible_change.set_output_folder(self.folder_path)
        implausible_change.set_parameters(self.params)
        if self.active_tests['spike_value_statistical']:
            df = implausible_change.detect_spikes_statistical(df, self.time_column, relevant_measurements, self.information, self.original_length, suffix)
        if self.active_tests['implausible_change']:
            df = implausible_change.run(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)

        #Detect spike values
        spike_detection = qc_spike.SpikeDetector()
        spike_detection.set_output_folder(self.folder_path)
        spike_detection.set_parameters(self.params)
        self.information.append(['Various Spike detection approaches and their outcomes:'])
        if self.active_tests['cotede_spikes']:
            df = spike_detection.remove_spikes_cotede(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)
        if self.active_tests['cotede_improved_spikes']:
            df = spike_detection.remove_spikes_cotede_improved(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)
        if self.active_tests['selene_spikes']:
            df = spike_detection.selene_spike(df, relevant_measurements, self.time_column, self.segment_column, f'poly_interpolated_data{suffix}', self.information, self.original_length, suffix)
        if self.active_tests['selene_improved_spikes']:
            df = spike_detection.selene_spike_detection(df, relevant_measurements, self.time_column, f'spline_fitted_data{suffix}', self.information, self.original_length, suffix)
        if self.active_tests['harmonic_detected_spikes']:
            df = spike_detection.remove_spikes_harmonic(df, f'poly_fitted_data{suffix}', self.adapted_meas_col_name, self.time_column, self.information, self.original_length, suffix)
        if self.active_tests['ml_detected_spikes']:
            df = spike_detection.remove_spikes_ml(df, f'poly_fitted_data{suffix}', self.adapted_meas_col_name, self.time_column, self.information, self.original_length, suffix)
        
        #Detect shifts & deshift values
        shift_detection = qc_shifts.ShiftDetector()
        shift_detection.set_output_folder(self.folder_path)
        shift_detection.set_parameters(self.params)
        if self.active_tests['shifted_ruptures']:
            df = shift_detection.detect_shifts_ruptures(df, relevant_measurements, self.time_column, f'poly_interpolated_data{suffix}', self.segment_column, self.information, self.original_length, suffix)
        if self.active_tests['shifted_value']:
            df = shift_detection.detect_shifts_statistical(df, f'poly_interpolated_data{suffix}', self.time_column, relevant_measurements, self.segment_column, self.information, self.original_length, suffix)

        #Check what unsupervised ML would do
        self.unsupervised_outlier_detection(df, relevant_measurements, f'poly_interpolated_data{suffix}', self.time_column, self.segment_column, suffix)

        return df


    def set_global_timestamp(self, data):
        """
        Create a constant timeseries based on most frequent timestep in measurement and align measurements to it. This will introduce a lot of new NaNs for periods higher resolution.

        Input:
        - main dataframe [pandas df]
        """
        #Generate a new ts in 1 min timestamp
        start_time = data[self.time_column].iloc[0]
        end_time = data[self.time_column].iloc[-1]
        ts_full = pd.date_range(start= start_time, end= end_time, freq=self.min_timestep).to_frame(name=self.time_column).reset_index(drop=True)

        #Merge df based on timestamp
        df_meas_long = pd.merge(ts_full, data, on=self.time_column, how = 'outer')
        df_meas_long['resolution'] = df_meas_long['resolution'].bfill()
        #Get mask for the new introduced NaNs based on missing_values mask fom before (new NaNs = True)
        df_meas_long['missing_values'] = df_meas_long['missing_values'].fillna(False).infer_objects(copy=False)
        df_meas_long['incorrect_format'] = df_meas_long['incorrect_format'].fillna(False).infer_objects(copy=False)

        #This information is not needed for now
        #df_meas_long['missing_values_padding'] = np.where(df_meas_long['missing_values'].isna(), True, False)
        #df_meas_long.loc[df_meas_long['missing_values'] == True, 'missing_values_padding'] = False

        print('The new ts is',len(df_meas_long),'entries long.')
        self.information.append(['The new ts is',len(df_meas_long),'entries long.'])

        #plot with new 1-min ts for visual analysis
        self.helper.plot_df(df_meas_long[self.time_column], df_meas_long[self.measurement_column],'Water Level','Timestamp ','Measured water level in constant timestamp')
        self.helper.plot_df(df_meas_long[self.time_column][33300:33400], df_meas_long[self.measurement_column][33300:33400],'Water Level','Timestamp ','Measured water level in constant timestamp (zoom)')
        #self.helper.zoomable_plot_df(df_meas_long[self.time_column][:33600], df_meas_long_filled[self.measurement_column][:33600],'Water Level','Timestamp ', 'Measured water level time','measured water level time')

        return df_meas_long


    def short_bad_measurement_periods(self, data, data_column, segment_column, suffix):
        """
        Check if segments are very short or contain a lot of NaN values. If yes, drop those segments as bad segments.

        Input:
        -Main dataframe [pandas df]
        -Column name of measurement series of interest [str]
        -Column name of segmentation information [str]
        -suffix: ending for columns and graphs in order to run in different modes [str]
        """
        #If segment is short or lot of NaNs, drop it
        self.threshold_short_bad_segment = self.params['threshold_short_bad_segment']
        self.threshold_unusabel_segment_nan = self.params['threshold_empty_bad_segment']

        #for extractinglooping over segments
        data[f'short_bad_measurement_series{suffix}'] = False
        shift_points = (data[segment_column] != data[segment_column].shift())
        z = 0

        for i in range(0,len(data[segment_column][shift_points]), 1):  
            start_index = data[segment_column][shift_points].index[i]
            if i == len(data[segment_column][shift_points])-1:
                end_index = len(data)
            else:
                end_index = data[segment_column][shift_points].index[i+1]
            if data[segment_column][start_index] == 0:
                self.helper.plot_df(data[self.time_column][start_index:end_index], data[data_column][start_index:end_index],'Water Level', 'Timestamp',f'Segment graph {start_index} -{suffix}')
                #test_df = data[(data[time_column].dt.year == 2007) & (data[time_column].dt.month == 9)]
                #self.helper.plot_two_df_same_axis(test_df[time_column],test_df[data_column],'Water Level', 'Water Level', test_df[segment_column], 'Timestamp', 'Segment',f'Test Graph 0')
                print(f'Segment is {end_index - start_index} entries long.')
                print(np.sum(~np.isnan(data[data_column][start_index:end_index]))/len(data[data_column][start_index:end_index]))
                min = builtins.max(0,(start_index-20))
                max = builtins.min(len(data),(end_index+20))
                if end_index - start_index < self.threshold_short_bad_segment:
                    print(f'This bad period sarts with index {start_index}.')
                    data.loc[start_index:end_index, f'short_bad_measurement_series{suffix}'] = True
                    data.loc[start_index:end_index, data_column] = np.nan
                    data.loc[start_index:end_index, segment_column] = 1
                    z += 1
                    self.helper.plot_df(data[self.time_column][min:max], data[self.measurement_column][min:max],'Water Level', 'Timestamp', f'Bad and short periods (monthly) - Graph {i} -{suffix}')
                    self.helper.plot_df(data[self.time_column][min:max], data[data_column][min:max],'Water Level', 'Timestamp', f'Bad and short periods (monthly)- Cleaned - Graph{i} -{suffix}')
                elif np.sum(~np.isnan(data[data_column][start_index:end_index]))/len(data[data_column][start_index:end_index]) < self.threshold_unusabel_segment_nan:
                    print(f'This bad period sarts with index {start_index}.')
                    data.loc[start_index:end_index, f'short_bad_measurement_series{suffix}'] = True
                    data.loc[start_index:end_index, data_column] = np.nan
                    data.loc[start_index:end_index, segment_column] = 1
                    z += 1
                    self.helper.plot_df(data[self.time_column][min:max], data[self.measurement_column][min:max],'Water Level', 'Timestamp', f'Bad and empty periods (monthly) - Graph {i} -{suffix}')
                    self.helper.plot_df(data[self.time_column][min:max], data[data_column][min:max],'Water Level', 'Timestamp', f'Bad and empty periods (monthly)- Cleaned - Graph{i} -{suffix}')
        print(f"There are {z} bad segments in this timeseries.")
        self.information.append([f"There are {z} bad segments in this time series."])

        #for segment
        shift_points = (data[segment_column] != data[segment_column].shift())
        print(f'Now there are still {(data[segment_column][shift_points]==0).sum()} segments with measurements in this measurement series.')
        self.information.append([f'Now there are still {(data[segment_column][shift_points]==0).sum()} segments with measurements in this measurement series.'])

        return data


    def save_to_txt(self):
        """
        Save the print statements from each QC test to a common txt-file. This can be used later on to compare between stations and set-ups.
        """

        # Filepath to save the text file
        filename = f"QC_summary_{self.station}.txt"
        file_path = os.path.join(self.folder_path, filename)

        # Save the list to a .txt file
        with open(file_path, "w") as file:
            for list in self.information:
                 # Write each sublist as a row
                file.write(" ".join(map(str, list)) + "\n")
                # Add an empty line after each row
                file.write("\n")

        print(f"QC test statements have been saved to {file_path}")


    def unsupervised_outlier_detection(self, df, data_column_name, interpolated_data_colum, time_column, segment_column, suffix):
        """
        Run a simple unsupervised ML algorithm to see how it performs in grouping the measurments.

        Input:
        -Main dataframe [pandas df]
        -Column name of measurement series of interest [str]
        -Column name of filled series [str]
        -Column name of time & date information [str]
        -Column name of segmentation information [str]
        -suffix: ending for columns and graphs in order to run in different modes [str]
        """
        #one high-high or low-low cycle in tide for slicing ts for unsupervised ML test
        self.window_size = self.params['window_size_slicing_unsupervised_ml']

        shift_points = (df[segment_column] != df[segment_column].shift())
        df[f'unsupervised_ml_outliers{suffix}'] = False
        df['test'] = df[data_column_name].copy()

        for i in range(0,len(df[segment_column][shift_points]), 1):
            start_index = df[segment_column][shift_points].index[i]
            if i == len(df[segment_column][shift_points])-1:
                end_index = len(df)
            else:
                end_index = df[segment_column][shift_points].index[i+1]
            if df[segment_column][start_index] == 0:
                relev_df = df[start_index:end_index]
                #true_anomalies = relev_df[true_anomalies_column]

                # Initialize a padded array
                padding = self.window_size // 2
                padded_values = np.pad(relev_df[interpolated_data_colum].ffill(), pad_width=padding, mode='edge') 
                
                # Prepare data for Isolation Forest including reelv features
                X = []
                for i in range(len(relev_df)):
                    content = padded_values[i:i + self.window_size]
                    # Extract features (mean, std, min, max)
                    #mean = np.array([np.mean(content)])
                    #std = np.array([np.std(content)])
                    #min_val = np.array([np.min(content)])
                    #max_val = np.array([np.max(content)])
                    #features =  np.append(content, [mean, std, min_val, max_val])
                    X.append(content)
                X = np.array(X)

                # Grid search for optimal contamination
                #contamination_values = np.linspace(0.001, 0.1, 10)  # Test contamination levels from 0.1% to 10%
                #best_contamination = None
                #best_precision = 0

                #for contamination in contamination_values:
                #    isolation_forest = IsolationForest(contamination=contamination, n_estimators=200, random_state=42)
                #    anomaly_score = isolation_forest.fit_predict(X)
                        
                    # Convert anomaly scores to binary (1 for anomalies, 0 for normal)
                #    predicted_anomalies = (anomaly_score == -1).astype(int)
                        
                    # Compute precision score (you can also use recall or F1-score)
                #    precision = precision_score(true_anomalies, predicted_anomalies, zero_division=0)
                        
                #    if precision > best_precision:
                #        best_precision = precision
                #        best_contamination = contamination
                #        print(best_contamination, best_precision)

                # Fit Isolation Forest
                model = IsolationForest(contamination=0.05, n_estimators=200, random_state=42)  # Adjust contamination as needed
                anomaly = model.fit_predict(X)

                # Identify anomalies
                anomalies = relev_df[anomaly == -1]

                #Drop non-nan indices
                relev_index = anomalies.index
                non_nan_indices = relev_df[~relev_df[data_column_name].isna()].index.to_numpy()
                common_indices = relev_index.intersection(non_nan_indices)
                df.loc[common_indices, f'unsupervised_ml_outliers{suffix}'] = True

                # Visualization
                title = 'Anomaly Detection with Isolation Forest (Using Interpolation)'
                plt.figure(figsize=(12, 6))
                plt.plot(relev_df[time_column], relev_df[data_column_name], label='Time Series', color='blue', marker='o',  markersize=1)
                plt.scatter(anomalies[time_column], anomalies[data_column_name], color='red', label='Anomalies', zorder=1)
                plt.legend()
                plt.title(title)
                plt.xlabel('Time')
                plt.ylabel('Water Level')
                plt.tight_layout()
                plt.savefig(os.path.join(self.folder_path,f"{title}- Date: {relev_df[time_column].iloc[0]} -{suffix}.png"),  bbox_inches="tight")
                plt.close()

        #Analyse spike detectionunsupervised ML outcomes
        df['test'] = np.where(df[f'unsupervised_ml_outliers{suffix}'], np.nan, df['test'])
        true_indices = df[f'unsupervised_ml_outliers{suffix}'][df[f'unsupervised_ml_outliers{suffix}']].index
        #More plots
        if true_indices.any():
            max_range = builtins.min(31, len(true_indices))
            for i in range(1, max_range):
                min = builtins.max(0,(true_indices[i]-2000))
                max = builtins.min(len(df), min+4000)
                self.helper.plot_two_df_same_axis(df[time_column][min:max], df['test'][min:max],'Water Level', 'Water Level (corrected)', df[data_column_name][min:max], 'Timestamp ', 'Water Level (all)', f'Unsupervised ML Graph {i} -{suffix}')
                
        print('There are', len(true_indices),'incorrect values in the timeseries based on the unsupervised ML step.')
        self.information.append([f'There are {len(true_indices)} incorrect values in the timeseries based on the unsupervised ML step.'])

        del df['test']

