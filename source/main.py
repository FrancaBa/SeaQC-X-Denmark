###############################################################################################
## Written by frb for GronSL project (2024-2025)                                             ##
## This is the main script for the QC. From here the different tests and methods are called. ##
###############################################################################################

import os
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import builtins
import math

from functools import reduce

import source.helper_methods as helper
import source.data_extractor_monthly as extractor
import source.qc_test_results_to_mask as qc_test_results_to_mask
import source.helper_advanced_graphs as helper_advanced_graphs
import source.helper_tidal_signal_generation as helper_tidal_signal_generation

import source.various_qc_tests.qc_spike as qc_spike
import source.various_qc_tests.qc_interpolation_detector as qc_interpolated
import source.various_qc_tests.qc_shifts as qc_shifts
import source.various_qc_tests.qc_filling_missing_data as qc_fill_data
import source.various_qc_tests.qc_marker_probably_good as qc_prob_good
import source.various_qc_tests.qc_implausible_changes as qc_implausible_change
import source.various_qc_tests.qc_global_outliers as qc_global_outliers
import source.various_qc_tests.qc_stuck_values as qc_stuck_values
import source.various_qc_tests.qc_ml_detection as qc_ml_detector

class QualityFlagger():
    
    def __init__(self):

        self.helper = helper.HelperMethods()
        self.min_timestep = 1

        #Empty element to store different texts about QC tests and save as a summary document
        self.information = []

    def set_station(self, station):
        self.station = station
        self.information.append(['The following text summarizes the QC performed on measurements from ',station,':'])
    
    #provids details on station like coordinates
    def set_gauge_details(self, gauge_details_path):
        self.gauge_details_path = gauge_details_path

    #load config.json file to generate bitmask and flags after QC tests and to load threshold/parameters used for the QC tests
    def load_config_json(self, json_path):

        # Open and load JSON file containing the quality flag classification
        with open(json_path, 'r') as file:
            config_data = json.load(file)

        #Defines which flags are generated by the script
        self.qc_classes = config_data['qc_flag_classification']
        #Defines which bits are generated by the script
        self.qc_bit_definition = config_data['qc_binary_classification']
        #Defines which parameter potentially are used by the script (later on the timestep decides on the chosen constants)
        self.all_params = config_data['parameter_definition']
        #Defines which tests are active in the script
        self.active_tests = config_data['qc_active_tests']
        #Defines if certain tests should be carried out on detided data as well
        self.detide_mode = config_data['detide_mode']

    #Create output folder to save results
    def set_output_folder(self, folder_path):

        self.folder_path = folder_path

        #generate output folder for graphs and other docs
        if not os.path.exists(self.folder_path):
            os.makedirs(self.folder_path)

        self.helper.set_output_folder(self.folder_path)

    #Set input table to correct heading
    def set_column_names(self, time_column, measurement_column, qc_column):
        self.time_column = time_column
        self.measurement_column = measurement_column
        self.qc_column = qc_column

    #Dummy value for NaN-values in measurement series
    def set_missing_value_filler(self, missing_meas_value):
        self.missing_meas_value = missing_meas_value 

    #Set path to training dataset used for ML test for small-scale anomalies
    def set_ml_training_data(self, path1, path2):
        self.datadir = path1
        self.tidal_path = path2

    def import_data(self, path, file):
        """
        Importing the data from csv to df. And some first pre-processing:
        -Extracting areas of interest to csv filea. 
        -Fix timestamp column and column names 
        -Get resolution between different measurements
        -Plots for basic understanding
        -Remove invalid characters in measurement period
        -Set missing values to NaN
        Already contains the format check and setting invalid formats to NaN (2 QC tests).

        Input:
        - path to file [str]
        - filename including ending [str]
        """
        if file.endswith(".csv"):
            #Open .csv file and fix the column names
            self.df_meas = pd.read_csv(os.path.join(path,file), sep=",", header=0)
            self.df_meas = self.df_meas[[self.time_column, self.measurement_column]]
            self.df_meas[self.time_column] = pd.to_datetime(self.df_meas[self.time_column], format='%Y%m%d%H%M%S')
        elif file.endswith(".dat"):
            #Open .dat file and fix the column names
            self.df_meas = pd.read_csv(os.path.join(path,file), sep="\\s+", header=None, names=[self.time_column, self.measurement_column, self.qc_column])
            self.df_meas = self.df_meas[[self.time_column, self.measurement_column]]
            self.df_meas[self.time_column] = pd.to_datetime(self.df_meas[self.time_column], format='%Y%m%d%H%M%S')
        elif file.endswith(".txt"):
            #Open .txt file and fix the column names
            self.df_meas = pd.read_csv(os.path.join(path,file), sep="\\s+", header=None, names=[self.time_column, self.measurement_column])
            self.df_meas[self.measurement_column] = self.df_meas[self.measurement_column]/100 #conversion from cm to m
            self.df_meas[self.time_column] = pd.to_datetime(self.df_meas[self.time_column], format='%Y%m%d%H%M')
        else:
            raise Exception('Format of the input measurement series is unkown to this script.')
        
        #Raise an error for data without two decimats when in meters -> Thresholds are currently defined for two decimals. User can adapt it based on the need
        # Count decimals and find the most frequent one
        decimal_counts = self.df_meas[self.measurement_column].astype(str).str.split('.').str[1].str.len().fillna(0).astype(int)
        most_frequent_decimal_count = decimal_counts.mode()[0]
        if most_frequent_decimal_count < 2:
            raise Exception('The input data series does not have at least two decimals. The code and thresholds are currently tuned for timeseries with at least 2 decimals. Please, adapt the config.json to run this script with the current timeseries.')

        #Extract data for manual labelling
        data_extractor = extractor.DataExtractor()
        data_extractor.set_output_folder(self.folder_path, self.station)
        data_extractor.set_missing_value_filler(self.missing_meas_value)
        data_extractor.run(self.df_meas, self.time_column, self.measurement_column)

        #drop seconds
        self.df_meas[self.time_column] = self.df_meas[self.time_column].dt.round('min')
        self.information.append(['The analyzed series goes from ',self.df_meas[self.time_column].iloc[0].strftime('%Y-%m-%d %H:%M:%S'),' to ', self.df_meas[self.time_column].iloc[-1].strftime('%Y-%m-%d %H:%M:%S'), '.'])

        #Length original dataset
        self.original_length = len(self.df_meas)

        # Extract the resolution in seconds, minutes, etc.
        # Here, we extract the time difference in minutes
        self.df_meas['time_diff'] = self.df_meas[self.time_column].diff()
        self.df_meas['resolution'] = self.df_meas['time_diff'].dt.total_seconds()/60
        frequent_timestep = self.df_meas['resolution'].value_counts().head(4)
        frequent_timestep_int =[int(res) for res in frequent_timestep.index]
        self.information.append(['The three most frequent time steps in dataset are:', frequent_timestep])
        print('The three most frequent time steps in dataset are:', frequent_timestep)
        self.min_timestep_int = reduce(math.gcd, frequent_timestep_int)
        self.min_timestep = str(float(self.min_timestep_int)) + 'min'
        #Select correct parameters based on timestep
        self.params = self.all_params.get(self.min_timestep)
        self.information.append(['The series is converted to a ', self.min_timestep, ' time resolution.'])
        print('The series is converted to a ', self.min_timestep, ' time resolution.')

        self.df_meas.loc[self.df_meas['resolution'] > 3600, 'resolution'] = np.nan
        self.information.append(['The measurement series is ', len(self.df_meas),'data entries long.'])

        #Plot the original ts to get an understanding
        self.helper.plot_df(self.df_meas[self.time_column][-2000:-1000], self.df_meas['resolution'][-2000:-1000],'step size','Timestamp ','Time resolution (zoomed)')
        self.helper.plot_df(self.df_meas[self.time_column][33300:33320], self.df_meas['resolution'][33300:33320],'step size','Timestamp ','Time resolution (zoomed 2)')
        self.helper.plot_df(self.df_meas[self.time_column], self.df_meas['resolution'],'step size','Timestamp ','Time resolution')

        #Initial screening of measurements:
        #1. Invalid characters: Set non-float elements to NaN
        # Count the original NaN values
        original_nan_count = self.df_meas[self.measurement_column].isna().sum()
        altered_ts = self.df_meas[self.measurement_column].apply(lambda x: x if isinstance(x, float) else np.nan)
        self.df_meas['incorrect_format'] = np.where(altered_ts, False, True)
        self.df_meas[self.measurement_column] = altered_ts
        # Count the new NaN values after the transformation -> Where there any invalid characters?
        new_nan_count = self.df_meas[self.measurement_column].isna().sum() - original_nan_count
        print('The measurement series contained',new_nan_count,'invalid data entires.')
        self.information.append(['The measurement series contains',new_nan_count,'invalid data entries.'])
        
        #2. Replace the missing values with nan
        self.df_meas['missing_values'] = self.df_meas[self.measurement_column] == self.missing_meas_value
        self.df_meas.loc[self.df_meas[self.measurement_column] == self.missing_meas_value, self.measurement_column] = np.nan
        ratio = (self.df_meas['missing_values'].sum()/len(self.df_meas))*100
        print(f'The measurement series contains {self.df_meas['missing_values'].sum()} missing data entires. This is {ratio}% of the overall dataset.')
        self.information.append([f'The measurement series contains {self.df_meas['missing_values'].sum()} missing data entries. This is {ratio}% of the overall dataset.'])
                                
        #Plot for visualisation
        self.helper.plot_df(self.df_meas[self.time_column], self.df_meas[self.measurement_column],'Water Level','Timestamp ','Measured water level')
        
        #Plot distribution for analysis
        plt.hist(self.df_meas[self.measurement_column] , bins=3000, edgecolor='black', alpha=0.7)
        plt.title('Distribution of Values')
        plt.xlabel('Water Level')
        plt.ylabel('Frequency')
        plt.savefig(os.path.join(self.folder_path,"Distribuiton - WL measurements.png"),  bbox_inches="tight")
        plt.close() 

        #Subsequent line makes the code slow, only enable when needed
        #print('length of ts:', len(self.df_meas))
        #self.helper.zoomable_plot_df(self.df_meas[self.time_column], self.df_meas[self.measurement_column],'Water Level','Timestamp ', 'Measured water level','measured water level')

    """
    The methods above are to open and preprocess the needed information and data for the quality check.
    The run-method below is the core of the QC work. It calls the different QC steps, called detided_mode, converts the masks to bitmasks and assigns quality flags.
    """
    def run(self):
        #Set relevant column names
        self.adapted_meas_col_name = 'altered'
        suffix = '_detided'

        #Padding of ts to a homogenous timestep
        df = self.set_global_timestamp(self.df_meas)
        df[self.adapted_meas_col_name] = df[self.measurement_column]

        #Generate tide signal as needed for further assessment
        tidal_signal_construction = helper_tidal_signal_generation.TidalSignalGenerator()
        tidal_signal_construction.set_station(self.station)
        tidal_signal_construction.set_gauge_details_path(self.gauge_details_path)
        tidal_signal_construction.set_output_folder(self.folder_path)
        df = tidal_signal_construction.run(df, self.adapted_meas_col_name, self.time_column, self.information)
        
        #Runs the different steps in the QC algorithm
        df = self.run_qc(df, self.adapted_meas_col_name)

        #Run the different steps in the QC algorithm for detided data
        if self.detide_mode == True:
            self.information.append(['The QC steps are now performed again for detided data.'])
            #Export the new series to csv files for manual labelling
            data_extractor = extractor.DataExtractor()
            data_extractor.set_output_folder(self.folder_path, self.station)
            data_extractor.set_missing_value_filler(self.missing_meas_value)
            data_extractor.run(df, self.time_column, 'detided_series', suffix)
            #Carry out QC steps on detided series as well
            df = self.run_qc(df, 'detided_series',suffix)
        
        #Probably good data
        #Mark all data as probably good data if it is only a short measurement period between bad data
        if self.active_tests['probably_good_data']:
            prob_good = qc_prob_good.ProbablyGoodDataFlagger()
            prob_good.set_output_folder(self.folder_path)
            prob_good.set_parameters(self.params)
            df = prob_good.run(df, self.adapted_meas_col_name, self.time_column, self.information, self.original_length)

        #Convert information of passed and failed tests to flags
        conversion_bitmask = qc_test_results_to_mask.QualityMasking()
        conversion_bitmask.set_output_folder(self.folder_path)
        conversion_bitmask.set_station(self.station)
        conversion_bitmask.set_flags(self.qc_classes)
        conversion_bitmask.set_bitmask(self.qc_bit_definition)
        conversion_bitmask.set_active_tests(self.active_tests)
        df = conversion_bitmask.convert_boolean_to_bitmasks(df)
        df = conversion_bitmask.convert_boolean_to_bitmasks(df,suffix)
        df = conversion_bitmask.merge_bitmasks(df, self.detide_mode, self.information, suffix)
        df = conversion_bitmask.convert_bitmask_to_flags(df)
        conversion_bitmask.save_flagged_series(df, self.measurement_column, self.time_column, self.df_meas)

        #Save information from QC tests to txt file
        self.save_to_txt()

        #Make beautiful graphs for papers/presentations (not generalized)
        plotter = helper_advanced_graphs.GraphMaker()
        plotter.set_output_folder(self.folder_path)
        plotter.set_station(self.station)
        plotter.run(df, self.time_column, self.measurement_column)

    def run_qc(self, df, relevant_measurements, suffix=''):
        """
        As main QC test method, it calls the different steps taken in the QC approach. See commented text.

        Input:
        -Main dataframe [pandas df]
        -Column name of measurement series of interest [str]
        -suffix: ending for columns and graphs in order to run in different modes [str]
        """
        #Detect stuck values in ts
        stuck_values = qc_stuck_values.StuckValuesDetector()
        stuck_values.set_output_folder(self.folder_path)
        stuck_values.set_parameters(self.params)
        df = stuck_values.run(df, self.time_column, relevant_measurements, self.information, self.original_length, suffix)

        #Detect global outliers in ts
        global_outliers = qc_global_outliers.OutlierRemover()
        global_outliers.set_output_folder(self.folder_path)
        global_outliers.set_parameters(self.params, suffix)
        df = global_outliers.run(df, relevant_measurements, self.time_column, self.measurement_column, self.information, self.original_length, suffix)
        #df = global_outliers.run_zscore(df, relevant_measurements, self.time_column, self.measurement_column, self.information, self.original_length, suffix)

        #Detect interpolated values
        interpolated_qc = qc_interpolated.Interpolation_Detector()
        interpolated_qc.set_output_folder(self.folder_path)
        interpolated_qc.set_parameters(self.params)
        df = interpolated_qc.run_interpolation_detection(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)
       
        #Segmentation of ts in empty and measurement segments
        #Extract measurement segments and fill them accordingly
        self.segment_column = f'segments{suffix}'
        fill_data_qc = qc_fill_data.MissingDataFiller()
        fill_data_qc.set_output_folder(self.folder_path)
        fill_data_qc.set_parameters(self.params)
        df = fill_data_qc.segmentation_ts(df, relevant_measurements, self.segment_column, self.information, suffix)

        #Set short measurement periods between missing data periods as not trustworthy periods
        if suffix == '':
            df = self.short_bad_measurement_periods(df, relevant_measurements, self.segment_column, suffix)

        #Add continuous helper columns
        df = fill_data_qc.polynomial_fill_data_column(df, relevant_measurements, self.time_column, self.segment_column, suffix)
        df = fill_data_qc.polynomial_fitted_data_column(df, relevant_measurements, self.time_column, self.segment_column, f'poly_interpolated_data{suffix}', suffix)
        df = fill_data_qc.spline_fitted_measurement_column(df, relevant_measurements, self.time_column, self.segment_column, suffix)
        #fill_data_qc.compare_filled_measurements(df, self.time_column, self.segment_column, suffix)

        #Detect implausible change rate over period
        implausible_change = qc_implausible_change.ImplausibleChangeDetector()
        implausible_change.set_output_folder(self.folder_path)
        implausible_change.set_parameters(self.params)
        if self.active_tests['spike_value_statistical']:
            df = implausible_change.detect_spikes_statistical(df, self.time_column, relevant_measurements, self.information, self.original_length, suffix)
        if self.active_tests['implausible_change']:
            df = implausible_change.run(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)

        #Detect spike values
        spike_detection = qc_spike.SpikeDetector()
        spike_detection.set_output_folder(self.folder_path)
        spike_detection.set_parameters(self.params)
        self.information.append(['Various Spike detection approaches and their outcomes:'])
        if self.active_tests['cotede_spikes']:
            df = spike_detection.remove_spikes_cotede(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)
        if self.active_tests['cotede_improved_spikes']:
            df = spike_detection.remove_spikes_cotede_improved(df, relevant_measurements, self.time_column, self.information, self.original_length, suffix)
        if self.active_tests['selene_spikes']:
            df = spike_detection.selene_spike(df, relevant_measurements, self.time_column, self.segment_column, f'poly_interpolated_data{suffix}', self.information, self.original_length, suffix)
        if self.active_tests['selene_improved_spikes']:
            df = spike_detection.selene_spike_detection(df, relevant_measurements, self.time_column, f'spline_fitted_data{suffix}', self.information, self.original_length, suffix)
        if self.active_tests['harmonic_detected_spikes']:
            df = spike_detection.remove_spikes_harmonic(df, f'poly_fitted_data{suffix}', relevant_measurements, self.time_column, self.information, self.original_length, suffix)
        if self.active_tests['ml_detected_spikes']:
            df = spike_detection.remove_spikes_ml(df, f'poly_fitted_data{suffix}', relevant_measurements, self.time_column, self.information, self.original_length, suffix)
        
        #Detect shifts & deshift values
        shift_detection = qc_shifts.ShiftDetector()
        shift_detection.set_output_folder(self.folder_path)
        shift_detection.set_parameters(self.params)
        if self.active_tests['shifted_ruptures']:
            df = shift_detection.detect_shifts_ruptures(df, relevant_measurements, self.time_column, f'poly_interpolated_data{suffix}', self.segment_column, self.information, self.original_length, suffix)
        if self.active_tests['shifted_value']:
            df = shift_detection.detect_shifts_statistical(df, f'poly_interpolated_data{suffix}', self.time_column, relevant_measurements, self.segment_column, self.information, self.original_length, suffix)

        #Detect small-scale anomalies with ML with combined training approach
        if self.active_tests['ml_anomalies'] and suffix == '':
            training_strategy = 'combined'
            #select output folder
            output_path = os.path.join(self.folder_path,'ML Anomaly Detection')
            data_flagging_ml = qc_ml_detector.MLOutlierDetection()
            data_flagging_ml.set_output_folder(output_path)
            data_flagging_ml.set_column_names('Timestamp', 'value', 'label')
            data_flagging_ml.set_station(training_strategy)
            tidal_info = data_flagging_ml.set_tidal_components_file(self.tidal_path)
            dfs_station_subsets, dfs_training = data_flagging_ml.import_data(self.datadir, tidal_info)
            data_flagging_ml.run_combined_training(dfs_training, tidal_info)
            df = data_flagging_ml.add_features(df, df['tidal_signal'], relevant_measurements)
            prediction = data_flagging_ml.run_test_data(df)
            df[f'ml_anomalies{suffix}'] = prediction.astype(bool)

        return df


    def set_global_timestamp(self, data):
        """
        Create a constant timeseries based on most frequent timestep in measurement and their greatest common divisor. This will introduce a lot of new NaNs for periods higher resolution.

        Input:
        - main dataframe [pandas df]
        """
        #Generate a new ts in 1 min timestamp
        start_time = data[self.time_column].iloc[0]
        end_time = data[self.time_column].iloc[-1]
        ts_full = pd.date_range(start= start_time, end= end_time, freq=self.min_timestep).to_frame(name=self.time_column).reset_index(drop=True)

        #Merge df based on timestamp
        df_meas_long = pd.merge(ts_full, data, on=self.time_column, how = 'outer')
        df_meas_long['resolution'] = df_meas_long['resolution'].bfill()
        #Get mask for the new introduced NaNs based on missing_values mask fom before (new NaNs = True)
        df_meas_long['missing_values'] = df_meas_long['missing_values'].fillna(False).infer_objects(copy=False)
        df_meas_long['incorrect_format'] = df_meas_long['incorrect_format'].fillna(False).infer_objects(copy=False)

        print('The new ts is',len(df_meas_long),'entries long.')
        self.information.append(['The new ts is',len(df_meas_long),'entries long.'])

        #plot with new 1-min ts for visual analysis
        self.helper.plot_df(df_meas_long[self.time_column], df_meas_long[self.measurement_column],'Water Level','Timestamp ','Measured water level in constant timestamp')
        self.helper.plot_df(df_meas_long[self.time_column][33300:33400], df_meas_long[self.measurement_column][33300:33400],'Water Level','Timestamp ','Measured water level in constant timestamp (zoom)')
        #self.helper.zoomable_plot_df(df_meas_long[self.time_column][:33600], df_meas_long_filled[self.measurement_column][:33600],'Water Level','Timestamp ', 'Measured water level time','measured water level time')

        return df_meas_long


    def short_bad_measurement_periods(self, data, data_column, segment_column, suffix):
        """
        Check if segments are very short or contain a lot of NaN values. If yes, drop those segments as bad segments.

        Input:
        -Main dataframe [pandas df]
        -Column name of measurement series of interest [str]
        -Column name of segmentation information [str]
        -suffix: ending for columns and graphs in order to run in different modes [str]
        """
        #If segment is short or lot of NaNs, drop it
        self.threshold_short_bad_segment = self.params['threshold_short_bad_segment']
        self.threshold_unusabel_segment_nan = self.params['threshold_empty_bad_segment']

        #for extractinglooping over segments
        data[f'short_bad_measurement_series{suffix}'] = False
        shift_points = (data[segment_column] != data[segment_column].shift())
        z = 0

        for i in range(0,len(data[segment_column][shift_points]), 1):  
            start_index = data[segment_column][shift_points].index[i]
            if i == len(data[segment_column][shift_points])-1:
                end_index = len(data)
            else:
                end_index = data[segment_column][shift_points].index[i+1]
            if data[segment_column][start_index] == 0:
                self.helper.plot_df(data[self.time_column][start_index:end_index], data[data_column][start_index:end_index],'Water Level', 'Timestamp',f'Segment graph {start_index} -{suffix}')
                print(f'Segment is {end_index - start_index} entries long.')
                print(np.sum(~np.isnan(data[data_column][start_index:end_index]))/len(data[data_column][start_index:end_index]))
                min = builtins.max(0,(start_index-20))
                max = builtins.min(len(data),(end_index+20))
                if end_index - start_index < self.threshold_short_bad_segment:
                    print(f'This bad period sarts with index {start_index}.')
                    data.loc[start_index:end_index, f'short_bad_measurement_series{suffix}'] = True
                    data.loc[start_index:end_index, data_column] = np.nan
                    data.loc[start_index:end_index, segment_column] = 1
                    z += 1
                    self.helper.plot_df(data[self.time_column][min:max], data[self.measurement_column][min:max],'Water Level', 'Timestamp', f'Bad and short periods (monthly) - Graph {i} -{suffix}')
                    self.helper.plot_df(data[self.time_column][min:max], data[data_column][min:max],'Water Level', 'Timestamp', f'Bad and short periods (monthly)- Cleaned - Graph{i} -{suffix}')
                elif np.sum(~np.isnan(data[data_column][start_index:end_index]))/len(data[data_column][start_index:end_index]) < self.threshold_unusabel_segment_nan:
                    print(f'This bad period sarts with index {start_index}.')
                    data.loc[start_index:end_index, f'short_bad_measurement_series{suffix}'] = True
                    data.loc[start_index:end_index, data_column] = np.nan
                    data.loc[start_index:end_index, segment_column] = 1
                    z += 1
                    self.helper.plot_df(data[self.time_column][min:max], data[self.measurement_column][min:max],'Water Level', 'Timestamp', f'Bad and empty periods (monthly) - Graph {i} -{suffix}')
                    self.helper.plot_df(data[self.time_column][min:max], data[data_column][min:max],'Water Level', 'Timestamp', f'Bad and empty periods (monthly)- Cleaned - Graph{i} -{suffix}')
        print(f"There are {z} bad segments in this timeseries.")
        self.information.append([f"There are {z} bad segments in this time series."])

        #for segment
        shift_points = (data[segment_column] != data[segment_column].shift())
        print(f'Now there are still {(data[segment_column][shift_points]==0).sum()} segments with measurements in this measurement series.')
        self.information.append([f'Now there are still {(data[segment_column][shift_points]==0).sum()} segments with measurements in this measurement series.'])

        return data


    def save_to_txt(self):
        """
        Save the print statements from each QC test to a common txt-file as repot. This can be used later on to compare between stations and set-ups.
        """

        # Filepath to save the text file
        filename = f"QC_summary_{self.station}.txt"
        file_path = os.path.join(self.folder_path, filename)

        # Save the list to a .txt file
        with open(file_path, "w") as file:
            for list in self.information:
                 # Write each sublist as a row
                file.write(" ".join(map(str, list)) + "\n")
                # Add an empty line after each row
                file.write("\n")

        print(f"QC test statements have been saved to {file_path}")
